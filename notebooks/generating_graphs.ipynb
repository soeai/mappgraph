{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generating_graphs_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lcDoZ0o2Za3"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcQq7a-99v_n"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLn9jLJy9zmn",
        "outputId": "081d348b-cdc6-4421-eba5-9b03107f948e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI3bEwTqqLcN"
      },
      "source": [
        "root_path = '/content/drive/My Drive/AI PROJECTS/MAppGraph/dataset'\n",
        "# root_path = '/content/drive/My Drive/Paper Artifacts/new_data'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Y8LA582yMR"
      },
      "source": [
        "## Config setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRNn9HINruSH"
      },
      "source": [
        "duration = 5\n",
        "overlap = 3\n",
        "\n",
        "N = 20\n",
        "window = 10"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLIho-0Qk_G2"
      },
      "source": [
        "## Graph generator functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQVa02ScsHLZ"
      },
      "source": [
        "Basic reprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq3YFm7csHpo"
      },
      "source": [
        "def basic_reprocessing(df, N):\n",
        "  # remove unknown protocol\n",
        "  df = df[df['protocol'] != 'unknown']\n",
        "\n",
        "  # remove dns protocol\n",
        "  df = df[(df['source_port'] != 53) & (df['destination_port'] != 53) & \n",
        "        (df['source_port'] != 5353) & (df['destination_port'] != 5353) &\n",
        "        (df['source_port'] != 137) & (df['destination_port'] != 137) &\n",
        "        (df['source_port'] != 67) & (df['destination_port'] != 67) &\n",
        "        (df['source_port'] != 68) & (df['destination_port'] != 68) &\n",
        "        (df['source_port'] != 5355) & (df['destination_port'] != 5355)]\n",
        "  \n",
        "  # remove IP 239.255.255.250, protocol SSDP\n",
        "  df = df[(df['source_address'] != '239.255.255.250') & (df['destination_address'] != '239.255.255.250')]\n",
        "  \n",
        "  # get IP address and port number of the service\n",
        "  df['des_greater_src'] = df['destination_port'] - df['source_port']\n",
        "  df1 = df[df['des_greater_src'] > 0]\n",
        "  df2 = df[df['des_greater_src'] < 0]\n",
        "  df1['destination'] = df1['source_address']\n",
        "  df1['port'] = df1['source_port']\n",
        "  df1['outgoing'] = 0\n",
        "  df2['destination'] = df2['destination_address']\n",
        "  df2['port'] = df2['destination_port']\n",
        "  df2['outgoing'] = 1\n",
        "  df = pd.concat([df1, df2], ignore_index=True).sort_values(by='time').reset_index(drop=True)\n",
        "\n",
        "  # merge IP address into port (same tuple (IP, port) - same network destination)\n",
        "  df['IP_port'] = list(zip(df['destination'], df['port']))\n",
        "\n",
        "  df = df.drop(['source_address', 'destination_address', 'certificate', 'des_greater_src', 'source_port', 'destination_port', 'destination', 'port'], axis=1)\n",
        "\n",
        "  # get N network destinations that have the most packets\n",
        "  df_ = df.groupby(['IP_port'], as_index = False).agg({'length':['count']}).sort_values(by=[('length', 'count')], ascending=False)\n",
        "  destinations = df_[:N]['IP_port']\n",
        "\n",
        "  return df[df['IP_port'].isin(destinations)].reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No1iNg5tsMnT"
      },
      "source": [
        "Packet-based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyVlga9osKci"
      },
      "source": [
        "def pkt_reprocessing(df):\n",
        "  df = df.drop(['time', 'stream_id', 'protocol'], axis=1).reset_index(drop=True)\n",
        "  df = df.sort_values(by=['IP_port']).reset_index(drop=True)\n",
        "\n",
        "  # return 3 series of packet: outgoing, incoming, both\n",
        "  out_df = df[df['outgoing'] == 1].drop(['outgoing'], axis=1).reset_index(drop=True)\n",
        "  in_df = df[df['outgoing'] == 0].drop(['outgoing'], axis=1).reset_index(drop=True)\n",
        "  full_df = df.drop(['outgoing'], axis=1)\n",
        "\n",
        "  return out_df, in_df, full_df\n",
        "\n",
        "def percentile(n):\n",
        "    def percentile_(x):\n",
        "        return np.percentile(x, n)\n",
        "    percentile_.__name__ = 'percentile_%s' % n\n",
        "    return percentile_\n",
        "\n",
        "def extract_pkt_features(df, type=\"complete\"):\n",
        "  features_df = df.groupby(['IP_port'], as_index = False).\\\n",
        "    agg({'length':['max', 'min', 'mean', 'mad', 'std', 'var', 'skew', pd.DataFrame.kurt, 'count', \n",
        "                   percentile(10), percentile(20), percentile(30), percentile(40), percentile(50), \n",
        "                   percentile(60), percentile(70), percentile(80), percentile(90)],\n",
        "     })\n",
        "    \n",
        "  # rename columns\n",
        "  feature_names = ['max', 'min', 'mean', 'mad', 'std', 'var', 'skew', 'kurt', 'pkt_num', '10per', '20per', '30per', '40per', '50per', '60per', '70per', '80per', '90per']\n",
        "  features_df.columns = ['IP_port'] + [type + \"_\" + x for x in feature_names]\n",
        "\n",
        "    \n",
        "  return features_df"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jc9FP_7sPQ4"
      },
      "source": [
        "Flow-based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bise1EetsRmb"
      },
      "source": [
        "def flow_reprocessing(df):\n",
        "\n",
        "  df['protocol'] = df['protocol'] == 'tcp'\n",
        "  df['protocol'] = df['protocol'].astype('int')\n",
        "\n",
        "  # sort by stream_id, protocol, time\n",
        "  df = df.sort_values(by=['stream_id', 'protocol', 'time']).reset_index(drop=True)\n",
        "\n",
        "  # merge packets into flows\n",
        "  df =  df.groupby(['stream_id', 'protocol', 'IP_port'], as_index = False).\\\n",
        "              agg({'time':['min', 'max'],\n",
        "                    'length':['sum', 'count']})\n",
        "  \n",
        "  df = df.drop(['stream_id'], axis=1)\n",
        "\n",
        "  df.columns = ['protocol', 'IP_port', 'start', 'end', 'flow_length', 'pkt_num']\n",
        "  \n",
        "  # create duration of each flow\n",
        "  df['duration'] = df['end'] - df['start']\n",
        "  df = df.drop(['end', 'start'], axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def extract_flow_features(df):\n",
        "  features_df = df.groupby(['IP_port'], as_index = False).\\\n",
        "    agg({'protocol':['mean', 'count'],\n",
        "         'flow_length': ['mean'],\n",
        "          'pkt_num': ['mean'],\n",
        "         'duration': ['mean']\n",
        "     })\n",
        "    \n",
        "  # rename columns\n",
        "  features_df.columns = ['IP_port', 'protocol', 'flows_num', 'flow_length_mean', 'flow_pkt_num_mean', 'flow_duration_mean']\n",
        "\n",
        "  return features_df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYXNUFMfsTud"
      },
      "source": [
        "Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ERS8cs-sWM-"
      },
      "source": [
        "def weights_reprocessing(df):\n",
        "  df = df.drop(['stream_id', 'protocol', 'length', 'outgoing'], axis=1).reset_index(drop=True)\n",
        "  return df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1lVSv6qzf2T"
      },
      "source": [
        "def weight(window_indx1, window_indx2):\n",
        "  intersection = window_indx1.intersection(window_indx2)\n",
        "  union = window_indx1.union(window_indx2)\n",
        "  return len(intersection)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dcPhaD7saij"
      },
      "source": [
        "Merge packet-based and flow-based features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4kbYpcisbCN"
      },
      "source": [
        "def merge_features(df1, df2):\n",
        "  features_df = pd.merge(df1, df2, on=\"IP_port\")\n",
        "\n",
        "  # sort by complete pkt number\n",
        "  features_df = features_df.sort_values(by=\"complete_pkt_num\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "  return features_df"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbiYPKZZznYR"
      },
      "source": [
        "Main function to generate a graph from a traffic chunk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ua96rzsg-1"
      },
      "source": [
        "'''\n",
        "input: mobile traffic chunck as a dataframe, N (the maximum nodes kept to build one graph), window (number of seconds used to build weight between two nodes)\n",
        "output: two dataframe. One contains features of all nodes in a graph generated. The other contains weights between the nodes.\n",
        "'''\n",
        "def generate_features_weights(df, N, window):\n",
        "  df = basic_reprocessing(df, N)\n",
        "\n",
        "  #------------------------------ Generate features ------------------------------------\n",
        "  # generate packet-based features\n",
        "  out_df, in_df, complete_df = pkt_reprocessing(df)\n",
        "  complete_df = extract_pkt_features(complete_df)\n",
        "  out_df = extract_pkt_features(out_df, \"out\")\n",
        "  in_df = extract_pkt_features(in_df, \"in\")\n",
        "  pkt_features_df = pd.merge(pd.merge(complete_df, out_df, on=\"IP_port\"), in_df, on=\"IP_port\")\n",
        "  # replace NaN by 0\n",
        "  pkt_features_df = pkt_features_df.fillna(0)\n",
        "  \n",
        "  # generate flow-based features\n",
        "  flow_df = flow_reprocessing(df)\n",
        "  flow_features_df = extract_flow_features(flow_df)\n",
        "\n",
        "  # merge packet-based and flow-based features df into a single features df\n",
        "  features_df = merge_features(pkt_features_df, flow_features_df)\n",
        "\n",
        "  #------------------------------ Generate weights ------------------------------------\n",
        "  w_df = weights_reprocessing(df)\n",
        "  w_df['time'] = (w_df['time']//window).astype('int')\n",
        "  w_df = w_df.groupby('IP_port')['time'].agg(active= lambda x: set(x)).reset_index(drop=False)\n",
        "  \n",
        "  # create a dataframe of weights\n",
        "  destination1_list = []\n",
        "  destination2_list = []\n",
        "  weight_list = []\n",
        "  destinations = list(features_df['IP_port'])\n",
        "  active_destinations = set()\n",
        "\n",
        "  for i in range(len(destinations)):\n",
        "    for j in range(i+1, len(destinations)):\n",
        "      des1 = destinations[i]\n",
        "      des2 = destinations[j]\n",
        "      destination1_list.append(des1)\n",
        "      destination2_list.append(des2)\n",
        "      w = weight(w_df[w_df['IP_port'] == des1]['active'].values[0], w_df[w_df['IP_port'] == des2]['active'].values[0])\n",
        "      weight_list.append(w)\n",
        "      if w > 0:\n",
        "        active_destinations = active_destinations.union({des1, des2})\n",
        "  \n",
        "  # get inactive destinations to remove\n",
        "  inactive_destinations = list(set(destinations) - active_destinations)\n",
        "  \n",
        "  # create dataframe of edge weights\n",
        "  weights_df = pd.DataFrame(\n",
        "  {\n",
        "  \"source\": destination1_list,\n",
        "  \"target\": destination2_list,\n",
        "  \"weight\": weight_list,\n",
        "  }\n",
        "  )\n",
        "\n",
        "  weights_df = weights_df.sort_values(by=\"weight\", ascending=False, ignore_index=True)\n",
        "\n",
        "  # remove destinations that do not connect to any other destinations from features df\n",
        "  features_df = features_df[~features_df['IP_port'].isin(inactive_destinations)]\n",
        "  # add ip features\n",
        "  features_df['ip1'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[0]))\n",
        "  features_df['ip2'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[1]))\n",
        "  features_df['ip3'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[2]))\n",
        "  features_df['ip4'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[3]))\n",
        "\n",
        "  # remove destinations that do not connect to any other destinations from weights df\n",
        "  weights_df = weights_df[~weights_df['source'].isin(inactive_destinations) & ~weights_df['target'].isin(inactive_destinations)].reset_index(drop=True)\n",
        "\n",
        "  # min-max normalize weights\n",
        "  weights_df['weight'] = (weights_df['weight'] - weights_df['weight'].min())/(weights_df['weight'].max() - weights_df['weight'].min())\n",
        "\n",
        "  return features_df, weights_df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjW7cSVJ94-S"
      },
      "source": [
        "## Generate and save graphs from the traffic chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0VZEprxy6dJ"
      },
      "source": [
        "'''\n",
        "Input:\n",
        "- app_src: folder that contain all traffic chunks (samples) of the app that we want to generate graphs\n",
        "- filenames: list of filenames in app_src (training or testing)\n",
        "Output: Generate graphs and save the graphs for all set of parameters (N and window) for just one app.\n",
        "'''\n",
        "def generate_graphs_one_app(app_src, filenames):\n",
        "\n",
        "  feature_columns = ['IP_port', 'complete_max', 'complete_min', 'complete_mean', 'complete_mad', 'complete_std', 'complete_var', 'complete_skew','complete_kurt',\n",
        "                     'complete_pkt_num', 'complete_10per', 'complete_20per', 'complete_30per', 'complete_40per', 'complete_50per', 'complete_60per','complete_70per',\n",
        "                     'complete_80per', 'complete_90per', 'out_max', 'out_min', 'out_mean', 'out_mad', 'out_std', 'out_var', 'out_skew','out_kurt','out_pkt_num',\n",
        "                     'out_10per', 'out_20per', 'out_30per', 'out_40per', 'out_50per', 'out_60per', 'out_70per', 'out_80per', 'out_90per', 'in_max','in_min', \n",
        "                     'in_mean', 'in_mad', 'in_std', 'in_var', 'in_skew', 'in_kurt', 'in_pkt_num', 'in_10per', 'in_20per', 'in_30per', 'in_40per', 'in_50per',\n",
        "                     'in_60per', 'in_70per', 'in_80per', 'in_90per', 'protocol', 'flows_num', 'flow_length_mean', 'flow_pkt_num_mean', 'flow_duration_mean', \n",
        "                     'ip1', 'ip2', 'ip3', 'ip4', 'graph_id']\n",
        "  weight_columns = ['source', 'target', 'weight', 'graph_id']\n",
        "  \n",
        "  features_df = pd.DataFrame([], columns=feature_columns)\n",
        "  weights_df = pd.DataFrame([], columns=weight_columns)\n",
        "  graph_id = 0\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  # loop over all traffic chunks of one app\n",
        "  for filename in filenames:\n",
        "    path = os.path.join(app_src, filename)\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    df = df.sort_values(by='time')\n",
        "      \n",
        "    if df.empty:\n",
        "      print('EMPTY')\n",
        "      continue\n",
        "        \n",
        "    df['time'] = df['time'] - df['time'].iloc[0] # get base time\n",
        "\n",
        "    #------------- generate one graph -----------------\n",
        "    try:\n",
        "      node_data, weights = generate_features_weights(df, N, window)\n",
        "    except:\n",
        "      print('WRONG')\n",
        "      continue\n",
        "      \n",
        "    if weights.shape[0] > 1:\n",
        "      graph_id = graph_id + 1 \n",
        "      node_data['graph_id'] = graph_id\n",
        "      weights['graph_id'] = graph_id\n",
        "\n",
        "      #------------- add one graph into graphs of the app -----------------\n",
        "      features_df = pd.concat([features_df, node_data], ignore_index=True)\n",
        "      weights_df = pd.concat([weights_df, weights], ignore_index=True)\n",
        "      #--------------------------------------------------------------------\n",
        "\n",
        "  return [features_df, weights_df]\n",
        "      \n",
        "  print(\"================================================================END ONE APP================================================================\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kny4ma265qf"
      },
      "source": [
        "'''\n",
        "Input:\n",
        "- A set of parameter: Duration and overlap\n",
        "- Index: 0 if we want to generate graphs for training samples, 1 for testing samples \n",
        "'''\n",
        "def generate_graphs(duration, overlap, index=0):\n",
        "\n",
        "  # get train_test information\n",
        "  path = os.path.join(root_path, '%d_%d'%(duration, overlap), 'train_test_info.json')\n",
        "  with open(path, 'r') as f:\n",
        "    train_test_info = json.load(f)\n",
        "      \n",
        "  samples_folder = os.path.join(root_path, '%d_%d'%(duration, overlap), 'samples')\n",
        "\n",
        "  # initial a dictionary containing features and weights of graphs for all apps (app -> (features_df, weights_df))\n",
        "  graphs = dict()\n",
        "\n",
        "  idx = 0\n",
        "  for app in os.listdir(samples_folder):\n",
        "    idx += 1\n",
        "    print('Loading {} ... {}/{}'.format(app, idx, 101))\n",
        "    \n",
        "    app_src = os.path.join(samples_folder, app)\n",
        "    filenames = train_test_info[app][index]\n",
        "\n",
        "    graphs[app] = generate_graphs_one_app(app_src, filenames)\n",
        "  \n",
        "  return graphs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p667q9A7r44A"
      },
      "source": [
        "Generate graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "828kqOran8gn"
      },
      "source": [
        "# generate graphs for training dataset\n",
        "training_graphs = generate_graphs(duration, overlap, index=0)\n",
        "# generate graphs for testing dataset\n",
        "testing_graphs = generate_graphs(duration, overlap, index=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb4pKrlmJiGC"
      },
      "source": [
        "Standardize features of each node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmvW0FYTJbXE"
      },
      "source": [
        "# Get mean-std of each features in the training dataset\n",
        "\n",
        "# define the initial empty dataframe\n",
        "cols = ['IP_port', 'complete_max', 'complete_min', 'complete_mean', 'complete_mad', 'complete_std', 'complete_var', 'complete_skew',\n",
        "       'complete_kurt', 'complete_pkt_num', 'complete_10per', 'complete_20per', 'complete_30per', 'complete_40per', 'complete_50per', \n",
        "        'complete_60per', 'complete_70per', 'complete_80per', 'complete_90per', 'out_max', 'out_min', 'out_mean', 'out_mad', 'out_std',\n",
        "        'out_var', 'out_skew', 'out_kurt', 'out_pkt_num', 'out_10per', 'out_20per', 'out_30per', 'out_40per', 'out_50per', 'out_60per',\n",
        "        'out_70per', 'out_80per', 'out_90per', 'in_max', 'in_min', 'in_mean', 'in_mad', 'in_std', 'in_var', 'in_skew', 'in_kurt', \n",
        "        'in_pkt_num', 'in_10per', 'in_20per', 'in_30per', 'in_40per', 'in_50per', 'in_60per', 'in_70per', 'in_80per', 'in_90per', \n",
        "        'protocol', 'flows_num', 'flow_length_mean', 'flow_pkt_num_mean', 'flow_duration_mean', 'ip1', 'ip2', 'ip3', 'ip4',  'graph_id'\n",
        "       ]\n",
        "df = pd.DataFrame([], columns=cols)\n",
        "\n",
        "# loop over train graphs\n",
        "for app in training_graphs.keys():\n",
        "  df_ = training_graphs[app][0]\n",
        "  df = pd.concat([df, df_], axis=0)\n",
        "\n",
        "# save mean and std of all featurs as dictionary\n",
        "mean_std_dic = dict()\n",
        "for feature in df.columns:\n",
        "  if feature not in ['IP_port', 'ip1', 'ip2', 'ip3', 'ip4', 'protocol', 'graph_id']:\n",
        "    mean_std_dic[feature] = (df[feature].mean(), df[feature].std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR16oiFpKicw"
      },
      "source": [
        "'''\n",
        "Input: A dataframe containing features of nodes in a graph, dictionary contain mean-std of all features\n",
        "Output: A dataframe of features after standardization\n",
        "'''\n",
        "def standardize_features(df, mean_std_dic):\n",
        "  # standardize the features in dataframe\n",
        "  for feature in mean_std_dic.keys():\n",
        "    m, std = mean_std_dic[feature][0], mean_std_dic[feature][1]\n",
        "    df[feature] = (df[feature] - m)/std\n",
        "  \n",
        "    # normalize ip feature\n",
        "    df['ip1'] = df['ip1']/255\n",
        "    df['ip2'] = df['ip2']/255\n",
        "    df['ip3'] = df['ip3']/255\n",
        "    df['ip4'] = df['ip4']/255\n",
        "  \n",
        "  return df\n",
        "\n",
        "# Standardization\n",
        "for app in training_graphs.keys():\n",
        "  training_graphs[app][0] = standardize_features(training_graphs[app][0], mean_std_dic)\n",
        "  testing_graphs[app][0] = standardize_features(testing_graphs[app][0], mean_std_dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqmTs62zNCJX"
      },
      "source": [
        "Save graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CShBW3euNClm"
      },
      "source": [
        "def save_graphs(graphs, dataset='train_graphs_new'):\n",
        "  \n",
        "  #----------------------- create folder to save graphs ---------------------\n",
        "  saved_graph_folder = os.path.join(root_path, '%d_%d'%(duration, overlap), dataset)\n",
        "  if not os.path.exists(saved_graph_folder):\n",
        "    os.mkdir(saved_graph_folder)\n",
        "\n",
        "  N_folder = os.path.join(saved_graph_folder, 'N%d'%N)\n",
        "  if not os.path.exists(N_folder):\n",
        "    os.mkdir(N_folder)\n",
        "\n",
        "  window_folder = os.path.join(N_folder, 't%d'%window)\n",
        "  if not os.path.exists(window_folder):\n",
        "    os.mkdir(window_folder)\n",
        "      \n",
        "  for app in graphs.keys():\n",
        "    graph_app_folder = os.path.join(window_folder, app)      \n",
        "    if not os.path.exists(graph_app_folder):\n",
        "      os.mkdir(graph_app_folder)\n",
        "    \n",
        "    '''\n",
        "    Save graphs for the app as two csv files (features.csv and weights.csv)\n",
        "    '''\n",
        "    features_path = os.path.join(graph_app_folder, 'features.csv')\n",
        "    features_df = graphs[app][0]\n",
        "    weights_path = os.path.join(graph_app_folder, 'weights.csv')\n",
        "    weights_df = graphs[app][1]\n",
        "\n",
        "    features_df.to_csv(features_path)\n",
        "    weights_df.to_csv(weights_path)\n",
        "\n",
        "save_graphs(training_graphs, dataset='train_graphs_new')\n",
        "save_graphs(testing_graphs, dataset='test_graphs_new')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJmp5VxybpQQ"
      },
      "source": [
        "___________________________________________________________________________________"
      ]
    }
  ]
}